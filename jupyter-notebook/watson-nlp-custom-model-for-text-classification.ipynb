{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watson NLP custom model for text classification\n",
    "\n",
    "This is a simplified example from different resources.\n",
    "\n",
    "## Related resources\n",
    "\n",
    "This example reuses information from a tutorial called [Text classification using Watson NLP](https://developer.ibm.com/tutorials/text-classification-using-watson-nlp/) on IBM Developer. The tutorial  points to the Juypter notebook called [Classifying customer complaints using Watson NLP](https://github.com/ibm-build-lab/Watson-NLP/blob/main/ML/Text-Classification/Consumer%20complaints%20Classification.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare environment\n",
    "## 1.1 Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Import the requests library\n",
    "\n",
    "# OS\n",
    "import requests\n",
    "# Layout\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "# Data\n",
    "import json\n",
    "import pandas as pd\n",
    "# Watson\n",
    "import watson_nlp\n",
    "from watson_nlp.workflows.classification import Ensemble\n",
    "from watson_core.data_model.streams.resolver import DataStreamResolver\n",
    "from watson_nlp.blocks.classification.svm import SVM\n",
    "# Confusion matrix in sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "# Project\n",
    "from project_lib import Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wget\n",
    "!pip install os\n",
    "!pip install zipfile\n",
    "!pip install pathlib\n",
    "!pip install shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS\n",
    "import os\n",
    "import wget\n",
    "import pathlib\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Download example data and transform to `csv` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.2.1 Verify the path and list the existing variable types, and files\n",
    "directory = os.getcwd()\n",
    "arr = os.listdir(directory)\n",
    "print(\"The variable, arr is of type:\", type(arr))\n",
    "print(\"The variable, directory is of type:\", type(directory))\n",
    "print(\"Directory '% s' created\" % directory)\n",
    "print(\"List '% s' created\" % arr)\n",
    "# 1.2.2 Clean-up existing files\n",
    "path = directory\n",
    "for file_name in os.listdir(path):\n",
    "    # Construct full file path\n",
    "    file = path + \"/\" + file_name\n",
    "    if os.path.isfile(file):\n",
    "        print('Deleting file:', file)\n",
    "        os.remove(file)\n",
    "\n",
    "# 1.2.3 Deleting a non-empty folder\n",
    "dir_path = directory + \"/embedding_use_en_stock\"\n",
    "shutil.rmtree(dir_path, ignore_errors=True)\n",
    "print(\"Deleted '%s' directory successfully\" % dir_path)\n",
    "dir_path = directory + \"/text_stopwords_classification_ensemble_en_stock\"\n",
    "shutil.rmtree(dir_path, ignore_errors=True)\n",
    "print(\"Deleted '%s' directory successfully\" % dir_path)\n",
    "dir_path = directory + \"/syntax_izumo_en_stock\"\n",
    "shutil.rmtree(dir_path, ignore_errors=True)\n",
    "print(\"Deleted '%s' directory successfully\" % dir_path)\n",
    "dir_path = directory + \"/embedding_glove_en_stock\"\n",
    "shutil.rmtree(dir_path, ignore_errors=True)\n",
    "print(\"Deleted '%s' directory successfully\" % dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.4 Set the path for the download: Usage of Consumer complaint database to walk you through the process. (https://www.consumerfinance.gov/data-research/consumer-complaints/)\n",
    "# URL = \"https://files.consumerfinance.gov/ccdb/complaints.csv.zip\"\n",
    "# To avoid data problems, you can use the data available on \"ibm.box.com\":\n",
    "URL = \"https://ibm.box.com/shared/static/fbs5buv3iix9bbckaw5ojsjf4ntkwvcz.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.5 Download the data behind the URL\n",
    "response = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.6 Open the response into a new file called complaints.csv.zip\n",
    "if (URL == \"https://files.consumerfinance.gov/ccdb/complaints.csv.zip\" ):\n",
    "   open(\"complaints.csv.zip\", \"wb\").write(response.content)\n",
    "else :\n",
    "   open(\"complaints.csv\", \"wb\").write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.7 Unzip the downloaded file and verify that the file was unzipped\n",
    "if (URL == \"https://files.consumerfinance.gov/ccdb/complaints.csv.zip\" ):\n",
    "    filepath = directory + \"/complaints.csv.zip\"\n",
    "    with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(directory)\n",
    "    arr = os.listdir(directory)\n",
    "    csv_files = list(pathlib.Path(directory).glob('*.csv'))\n",
    "    print(\"Directory '% s' created\" % directory)\n",
    "    print(\"List '% s' created\" % arr)\n",
    "    print(\"Csv files '% s' created\" % csv_files)\n",
    "else :\n",
    "    csv_files = list(pathlib.Path(directory).glob('*.csv'))\n",
    "    print(\"Csv files '% s' created\" % csv_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Optimize the example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.1 Reduce model training time and quick analysis using \"frac\". (https://en.wikipedia.org/wiki/Fractional_part)\n",
    "filepath = directory + \"/complaints.csv\"\n",
    "complaint_df = pd.read_csv(filepath, error_bad_lines=False)\n",
    "\n",
    "if (URL == \"https://files.consumerfinance.gov/ccdb/complaints.csv.zip\" ):\n",
    "    value=0.0005\n",
    "    complaint_df = complaint_df.sample(frac=value)\n",
    "    print(\"Frac '% 1.4f' \" % value)\n",
    "else :\n",
    "    value=0.02\n",
    "    complaint_df = complaint_df.sample(frac=value)\n",
    "    print(\"Frac '% 1.4f' \" % value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.2 Look at all the product groups that are available in the data set because these are the classes that the classifier should predict from a given complaint text.\n",
    "complaint_df['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.3 Filter on the Product categories with a relevant number of samples and remove any other product category from further analysis because many classification algorithms work best if the training samples are equally split across the classes. If the data is unbalanced, algorithms might decide to favor classes with many samples to achieve an overall good result.\n",
    "train_test_df = complaint_df[(complaint_df['Product'] == 'Credit reporting, credit repair services, or other personal consumer reports') | \\\n",
    "                             (complaint_df['Product'] == 'Debt collection') | \\\n",
    "                             (complaint_df['Product'] == 'Mortgage') | \\\n",
    "                             (complaint_df['Product'] == 'Credit card or prepaid card') | \\\n",
    "                             (complaint_df['Product'] == 'Checking or savings account')\n",
    "                            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.4 List the first 5 test entries for the training\n",
    "train_test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.5 Split the data into training and test data (ratio: 80/20).\n",
    "# 80% training data\n",
    "train_orig_df = train_test_df.groupby('Product').sample(frac=0.8, random_state=6)\n",
    "print(\"Training data:\\n\")\n",
    "print(\"Number of training samples:\\n{}\".format(len(train_orig_df)))\n",
    "print(\"Samples by product group:\\n{}\".format(train_orig_df['Product'].value_counts()))\n",
    "\n",
    "# 20% test data\n",
    "test_orig_df = train_test_df.drop(train_orig_df.index)\n",
    "print(\"\\nTest data:\\n\")\n",
    "print(\"Number of test samples:\\n{}\".format(len(test_orig_df)))\n",
    "print(\"Samples by product group:\\n{}\".format(test_orig_df['Product'].value_counts()))\n",
    "\n",
    "# re-index after sampling\n",
    "train_orig_df = train_orig_df.reset_index(drop=True)\n",
    "test_orig_df = test_orig_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.6 Create the data in a JSON format. The training and test data is written to files. \n",
    "def prepare_data(df):\n",
    "       # only the text column and the target label *Product* are needed\n",
    "       df_out = df[['Consumer complaint narrative', 'Product']].reset_index (drop=True)\n",
    "       # rename to the identifiers expected by Watson NLP\n",
    "       df_out = df_out.rename(columns={\"Consumer complaint narrative\": \"text\", 'Product': 'labels'})\n",
    "       # the label column should be an array (although we have only one label per complaint)\n",
    "       df_out['labels'] = df_out['labels'].map(lambda label: [label,])\n",
    "       return df_out\n",
    "\n",
    "train_orig_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = prepare_data(train_orig_df)\n",
    "# Clean all 'NaN'\n",
    "train_df.dropna(subset=['text'], how='all', inplace=True)\n",
    "train_file = directory + \"/train_data.json\"\n",
    "train_df.to_json(train_file, orient='records')\n",
    "\n",
    "test_df = prepare_data(test_orig_df)\n",
    "# Clean all 'NaN'\n",
    "test_df.dropna(subset=['text'], how='all', inplace=True)\n",
    "test_file = directory + \"/test_data.json\"\n",
    "test_df.to_json(test_file, orient='records')\n",
    "\n",
    "json_files = list(pathlib.Path(directory).glob('*.json'))\n",
    "print(\"JSON files '% s' created\" % json_files)\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.7 Show labels\n",
    "train_df.explode('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.3.8 Show labels\n",
    "test_df.explode('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.9 Show distribution\n",
    "plotly_template = pio.templates[\"plotly_dark\"]\n",
    "pio.templates[\"plotly_dark_custom\"] = pio.templates[\"plotly_dark\"]\n",
    "\n",
    "complaints_total_figure = px.bar(test_df.explode('labels')['labels'].value_counts())\n",
    "complaints_total_figure.update_layout(template=plotly_template,barmode='stack',title_text='Show test dataset', title_x=0.5)\n",
    "complaints_total_figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.9 Show distribution\n",
    "plotly_template = pio.templates[\"plotly_dark\"]\n",
    "pio.templates[\"plotly_dark_custom\"] = pio.templates[\"plotly_dark\"]\n",
    "\n",
    "complaints_total_figure = px.bar(train_df.explode('labels')['labels'].value_counts())\n",
    "complaints_total_figure.update_layout(template=plotly_template,barmode='stack',title_text='Show training dataset', title_x=0.5)\n",
    "complaints_total_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Load the syntax model and the USE embeddings because the SVM classifier block depends on the syntax block.\n",
    "\n",
    "# Syntax Model\n",
    "syntax_model = watson_nlp.load(watson_nlp.download('syntax_izumo_en_stock'))\n",
    "# USE Embedding Model\n",
    "use_model = watson_nlp.load(watson_nlp.download('embedding_use_en_stock'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Create data streams using several utility methods, \n",
    "# because classification blocks expect the training data \n",
    "# to be in data streams.\n",
    "\n",
    "training_data_file = train_file\n",
    "print (\"Training data file '%s'\"% train_file)\n",
    "\n",
    "# Create datastream from training data\n",
    "data_stream_resolver = DataStreamResolver(target_stream_type=list, expected_keys={'text': str, 'labels': list})\n",
    "training_data = data_stream_resolver.as_data_stream(training_data_file)\n",
    "\n",
    "# Create Syntax stream\n",
    "text_stream, labels_stream = training_data[0], training_data[1]\n",
    "syntax_stream = syntax_model.stream(text_stream)\n",
    "\n",
    "use_train_stream = use_model.stream(syntax_stream, doc_embed_style='raw_text')\n",
    "use_svm_train_stream = watson_nlp.data_model.DataStream.zip(use_train_stream, labels_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2.3 Train the classifier.\n",
    "# This can take several minutes!\n",
    "svm_model = SVM.train(use_svm_train_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Train an ensemble classification model with Watson NLP\n",
    "\n",
    "The ensemble model combines three classification models:\n",
    "\n",
    "* CNN\n",
    "* SVM with TF-IDF features\n",
    "* SVM with USE (Universal Sentence Encoder) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.1 Train the ensemble classifier.\n",
    "# This can take up to a 1 h!\n",
    "\n",
    "stopwords = watson_nlp.download_and_load('text_stopwords_classification_ensemble_en_stock')\n",
    "\n",
    "# Train the ensemble classifier. Note: This cell will run for several minutes. \n",
    "# To restrict the time, we limited the epochs to train the CNN classifier to 5. \n",
    "# This is an optional attribute - if not specified, the default will be 30 epochs.\n",
    "\n",
    "ensemble_model = Ensemble.train(train_file, 'syntax_izumo_en_stock', 'embedding_glove_en_stock', 'embedding_use_en_stock', stopwords=stopwords, cnn_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Save and download the model to the local machine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Please select `\"Insert project token\"` from the menu in the **Jupyter Notebook Editor of Watson Studio**. This will add the code below to the top of your notebook. You must run the code and then return to the current step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "# project = Project(project_id='YOUR_ID', project_access_token='YOUR_TOKEN')\n",
    "# pc = project.project_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.1 Store and load classification models\n",
    "# This can take several minutes!\n",
    "\n",
    "project.save_data('svm_model', data=svm_model.as_file_like_object(), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save_data('ensemble_model', data=ensemble_model.as_file_like_object(), overwrite=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `ensemble_model` to your local machine.\n",
    "\n",
    "* Select your project\n",
    "* Select `Asset types` -> `Data`\n",
    "* Select `Download` from the dropdown list for the `ensemble_model` \n",
    "\n",
    ">Note: The size of the file will be more than 1 Gig!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.1 Load svm model\n",
    "svm_model = watson_nlp.load(project.get_file('svm_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2.3.2 Load ensemble model\n",
    "model = project.get_file('ensemble_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model_new = watson_nlp.load(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Create a helper method to run both models on a single complaint and return the predicted product groups of both models.\n",
    "\n",
    "def predict_product(text):\n",
    "    # run syntax model first\n",
    "    syntax_result = syntax_model.run(text)\n",
    "    # run SVM model on top of syntax result\n",
    "    svm_preds = svm_model.run(use_model.run(syntax_result, doc_embed_style='raw_text'))\n",
    "    \n",
    "    predicted_svm = svm_preds.to_dict()[\"classes\"][0][\"class_name\"]\n",
    "    \n",
    "    ensemble_preds = ensemble_model.run(text)\n",
    "    predicted_ensemble = ensemble_preds.to_dict()[\"classes\"][0][\"class_name\"]\n",
    "    return (predicted_svm, predicted_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Run the models on the complete test data.\n",
    "\n",
    "text_col = 'Consumer complaint narrative'\n",
    "\n",
    "predictions = test_orig_df[text_col].apply(lambda text: predict_product(text))\n",
    "test_orig_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame.from_records(predictions, columns=('Predicted SVM', 'Predicted Ensemble'))\n",
    "predictions_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = test_orig_df[[text_col, \"Product\"]].merge(predictions_df, how='left', left_index=True, right_index=True)\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Creating and plotting a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual values\n",
    "actual = result_df['Product']\n",
    "# predicted values\n",
    "predicted_svm = result_df['Predicted SVM']\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "matrix = classification_report(actual,predicted_svm,labels=['Credit reporting, credit repair services, or other personal consumer reports',\n",
    "       'Mortgage', 'Credit card or prepaid card', 'Debt collection',\n",
    "       'Checking or savings account'])\n",
    "print('Classification report for SVM classifier: \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ensemble = result_df['Predicted Ensemble']\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "matrix = classification_report(actual,predicted_ensemble,labels=['Credit reporting, credit repair services, or other personal consumer reports',\n",
    "       'Mortgage', 'Credit card or prepaid card', 'Debt collection',\n",
    "       'Checking or savings account'])\n",
    "print('Classification report for Ensemble classifier: \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_confusion_df = pd.crosstab(result_df['Product'], result_df['Predicted SVM'], rownames=['Actual'], normalize='index')\n",
    "ensemble_confusion_df = pd.crosstab(result_df['Product'], result_df['Predicted Ensemble'], rownames=['Actual'], normalize='index')\n",
    "\n",
    "figure, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,7))\n",
    "# figure, ax1 = plt.subplots(ncols=1, figsize=(7,7))\n",
    "\n",
    "sn.heatmap(SVM_confusion_df, annot=True, cmap=\"YlGnBu\", ax=ax1, cbar=False)\n",
    "sn.heatmap(ensemble_confusion_df, annot=True, cmap=\"YlGnBu\", ax=ax2, cbar=False)\n",
    "ax1.title.set_text(\"SVM\")\n",
    "ax2.title.set_text(\"Ensemble\")\n",
    "ax2.set_yticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
