{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example reuses information from an example called [Text classification using watson NLP](https://developer.ibm.com/tutorials/text-classification-using-watson-nlp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare environment\n",
    "## 1.1 Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Import the requests library\n",
    "\n",
    "# OS\n",
    "import requests\n",
    "# Layout\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "# Data\n",
    "import pandas\n",
    "# Watson\n",
    "import watson_nlp\n",
    "from watson_nlp.workflows.classification import Ensemble\n",
    "from watson_core.data_model.streams.resolver import DataStreamResolver\n",
    "from watson_nlp.blocks.classification.svm import SVM\n",
    "# confusion matrix in sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sn\n",
    "# Project\n",
    "from project_lib import Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wget\n",
    "!pip install os\n",
    "!pip install zipfile\n",
    "!pip install pathlib\n",
    "!pip install shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS\n",
    "import os\n",
    "import wget\n",
    "import pathlib\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Download example data and transform to `csv` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.2.4 Verify the path and list the existing variables types, and files\n",
    "directory = os.getcwd()\n",
    "arr = os.listdir(directory)\n",
    "print(\"The variable, arr is of type:\", type(arr))\n",
    "print(\"The variable, directory is of type:\", type(directory))\n",
    "print(\"Directory '% s' created\" % directory)\n",
    "print(\"List '% s' created\" % arr)\n",
    "# 1.2.5. Clean-up existing files\n",
    "path = directory\n",
    "for file_name in os.listdir(path):\n",
    "    # construct full file path\n",
    "    file = path + \"/\" + file_name\n",
    "    if os.path.isfile(file):\n",
    "        print('Deleting file:', file)\n",
    "        os.remove(file)\n",
    "\n",
    "# 1.2.5 Deleting an non-empty folder\n",
    "dir_path = directory + \"/embedding_use_en_stock\"\n",
    "shutil.rmtree(dir_path, ignore_errors=True)\n",
    "print(\"Deleted '%s' directory successfully\" % dir_path)\n",
    "dir_path = directory + \"/text_stopwords_classification_ensemble_en_stock\"\n",
    "shutil.rmtree(dir_path, ignore_errors=True)\n",
    "print(\"Deleted '%s' directory successfully\" % dir_path)\n",
    "dir_path = directory + \"/syntax_izumo_en_stock\"\n",
    "shutil.rmtree(dir_path, ignore_errors=True)\n",
    "print(\"Deleted '%s' directory successfully\" % dir_path)\n",
    "dir_path = directory + \"/embedding_glove_en_stock\"\n",
    "shutil.rmtree(dir_path, ignore_errors=True)\n",
    "print(\"Deleted '%s' directory successfully\" % dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.1 Set the path for the download: Usage of Consumer complaint database to walk you through the process. (https://www.consumerfinance.gov/data-research/consumer-complaints/)\n",
    "URL = \"https://files.consumerfinance.gov/ccdb/complaints.csv.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.2 Download the data behind the URL\n",
    "response = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.3 Open the response into a new file called complaints.csv.zip\n",
    "open(\"complaints.csv.zip\", \"wb\").write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.5 Unzip the downloaded file and verify that the file was unzipped\n",
    "filepath = directory + \"/complaints.csv.zip\"\n",
    "with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory)\n",
    "arr = os.listdir(directory)\n",
    "csv_files = list(pathlib.Path(directory).glob('*.csv'))\n",
    "\n",
    "print(\"Directory '% s' created\" % directory)\n",
    "print(\"List '% s' created\" % arr)\n",
    "print(\"Csv files '% s' created\" % csv_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Optimize the example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.1 Reduce model training time and quick analysis using \"frac\". (https://en.wikipedia.org/wiki/Fractional_part)\n",
    "filepath = directory + \"/complaints.csv\"\n",
    "complaint_df = pandas.read_csv(filepath, error_bad_lines=False)\n",
    "complaint_df = complaint_df.sample(frac=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.2 Look at all of the product groups that are available in the data set because these are the classes that the classifier should predict from a given complaint text.\n",
    "complaint_df['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.3 Filter on the Product categories with a relevant number of samples and remove any other product category from further analysis because many classification algorithms work best if the training samples are equally split across the classes. If the data is unbalanced, algorithms might decide to favor classes with many samples to achieve an overall good result.\n",
    "train_test_df = complaint_df[(complaint_df['Product'] == 'Credit reporting, credit repair services, or other personal consumer reports') | \\\n",
    "                             (complaint_df['Product'] == 'Debt collection') | \\\n",
    "                             (complaint_df['Product'] == 'Mortgage') | \\\n",
    "                             (complaint_df['Product'] == 'Credit card or prepaid card') | \\\n",
    "                             (complaint_df['Product'] == 'Checking or savings account')\n",
    "                            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.4 List the first 5 test entries for the training\n",
    "train_test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.5 Split the data into training and test data (ratio: 80/20).\n",
    "# 80% training data\n",
    "train_orig_df = train_test_df.groupby('Product').sample(frac=0.8, random_state=6)\n",
    "print(\"Training data:\\n\")\n",
    "print(\"Number of training samples:\\n{}\".format(len(train_orig_df)))\n",
    "print(\"Samples by product group:\\n{}\".format(train_orig_df['Product'].value_counts()))\n",
    "\n",
    "# 20% test data\n",
    "test_orig_df = train_test_df.drop(train_orig_df.index)\n",
    "print(\"\\nTest data:\\n\")\n",
    "print(\"Number of test samples:\\n{}\".format(len(test_orig_df)))\n",
    "print(\"Samples by product group:\\n{}\".format(test_orig_df['Product'].value_counts()))\n",
    "\n",
    "# re-index after sampling\n",
    "train_orig_df = train_orig_df.reset_index(drop=True)\n",
    "test_orig_df = test_orig_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.6 Create the data in a JSON format. The training and test data is written to files. \n",
    "def prepare_data(df):\n",
    "       # only the text column and the target label *Product* are needed\n",
    "       df_out = df[['Consumer complaint narrative', 'Product']].reset_index (drop=True)\n",
    "       # rename to the identifiers expected by Watson NLP\n",
    "       df_out = df_out.rename(columns={\"Consumer complaint narrative\": \"text\", 'Product': 'labels'})\n",
    "       # the label column should be an array (although we have only one label per complaint)\n",
    "       df_out['labels'] = df_out['labels'].map(lambda label: [label,])\n",
    "       return df_out\n",
    "\n",
    "train_df = prepare_data(train_orig_df)\n",
    "# Clean all 'NaN'\n",
    "train_df.dropna(subset=['text'], how='all', inplace=True)\n",
    "train_file = directory + \"/train_data.json\"\n",
    "train_df.to_json(train_file, orient='records')\n",
    "\n",
    "test_df = prepare_data(test_orig_df)\n",
    "# Clean all 'NaN'\n",
    "test_df.dropna(subset=['text'], how='all', inplace=True)\n",
    "test_file = directory + \"/test data.json\"\n",
    "test_df.to_json(test_file, orient='records')\n",
    "\n",
    "json_files = list(pathlib.Path(directory).glob('*.json'))\n",
    "print(\"JSON files '% s' created\" % json_files)\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.7 Show labels\n",
    "train_df.explode('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.3.8 Show labels\n",
    "test_df.explode('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.9 Show distribution\n",
    "plotly_template = pio.templates[\"plotly_dark\"]\n",
    "pio.templates[\"plotly_dark_custom\"] = pio.templates[\"plotly_dark\"]\n",
    "\n",
    "complaints_total_figure = px.bar(test_df.explode('labels')['labels'].value_counts())\n",
    "complaints_total_figure.update_layout(template=plotly_template,barmode='stack',title_text='Show test dataset', title_x=0.5)\n",
    "complaints_total_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Load the syntax model and the USE embeddings because the SVM classifier block depends on the syntax block.\n",
    "\n",
    "# Syntax Model\n",
    "syntax_model = watson_nlp.load(watson_nlp.download('syntax_izumo_en_stock'))\n",
    "# USE Embedding Model\n",
    "use_model = watson_nlp.load(watson_nlp.download('embedding_use_en_stock'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Create data streams using several utility methods because classification blocks expect the training data to be in data streams.\n",
    "\n",
    "training_data_file = train_file\n",
    "print (\"Training data file '% s'\", train_file)\n",
    "\n",
    "# Create datastream from training data\n",
    "data_stream_resolver = DataStreamResolver(target_stream_type=list, expected_keys={'text': str, 'labels': list})\n",
    "training_data = data_stream_resolver.as_data_stream(training_data_file)\n",
    "\n",
    "# Create Syntax stream\n",
    "text_stream, labels_stream = training_data[0], training_data[1]\n",
    "syntax_stream = syntax_model.stream(text_stream)\n",
    "\n",
    "use_train_stream = use_model.stream(syntax_stream, doc_embed_style='raw_text')\n",
    "use_svm_train_stream = watson_nlp.data_model.DataStream.zip(use_train_stream, labels_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2.3 Train the classifier.\n",
    "# This can take several minutes!\n",
    "svm_model = SVM.train(use_svm_train_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Train an ensemble classification model with Watson NLP\n",
    "\n",
    "The ensemble model combines three classification models:\n",
    "\n",
    "* CNN\n",
    "* SVM with TF-IDF features\n",
    "* SVM with USE (Universal Sentence Encoder) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.1 Train the ensemble classifier.\n",
    "# This can take up to a 1 h!\n",
    "\n",
    "stopwords = watson_nlp.download_and_load('text_stopwords_classification_ensemble_en_stock')\n",
    "\n",
    "# Train the ensemble classifier. Note: This cell will run for several minutes. \n",
    "# To restrict the time, we limited the epochs to train the CNN classifier to 5. \n",
    "# This is an optional attribute - if not specified, the default will be 30 epochs.\n",
    "\n",
    "ensemble_model = Ensemble.train(train_file, 'syntax_izumo_en_stock', 'embedding_glove_en_stock', 'embedding_use_en_stock', stopwords=stopwords, cnn_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "project = Project(project_id='YOUR_IR', project_access_token='YOUR_TOKEN')\n",
    "pc = project.project_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.2 Store and load classification models\n",
    "# This can take several minutes!\n",
    "\n",
    "project.save_data('svm_model', data=svm_model.as_file_like_object(), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save_data('ensemble_model', data=ensemble_model.as_file_like_object(), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.3 Load svm model\n",
    "svm_model = watson_nlp.load(project.get_file('svm_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.4 Load ensemble model\n",
    "ensemble_model = watson_nlp.load(project.get_file('ensemble_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Create a helper method to run both models on a single complaint and return the predicted product groups of both models.\n",
    "\n",
    "def predict_product(text):\n",
    "    # run syntax model first\n",
    "    syntax_result = syntax_model.run(text)\n",
    "    # run SVM model on top of syntax result\n",
    "    svm_preds = svm_model.run(use_model.run(syntax_result, doc_embed_style='raw_text'))\n",
    "    \n",
    "    predicted_svm = svm_preds.to_dict()[\"classes\"][0][\"class_name\"]\n",
    "    \n",
    "    ensemble_preds = ensemble_model.run(text)\n",
    "    predicted_ensemble = ensemble_preds.to_dict()[\"classes\"][0][\"class_name\"]\n",
    "    return (predicted_svm, predicted_ensemble)\n",
    "#     return predicted_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Run the models on the complete test data.\n",
    "\n",
    "predictions = test_orig_df[text_col].apply(lambda text: predict_product(text))\n",
    "predictions_df = pd.DataFrame.from_records(predictions, columns=('Predicted SVM', 'Predicted Ensemble'))\n",
    "# predictions_df = pd.DataFrame.from_records(predictions, columns='Predicted SVM')\n",
    "\n",
    "result_df = test_orig_df[[text_col, \"Product\"]].merge(predictions_df, how='left', left_index=True, right_index=True)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Creating and plotting a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual values\n",
    "actual = result_df['Product']\n",
    "# predicted values\n",
    "predicted_svm = result_df['Predicted SVM']\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "matrix = classification_report(actual,predicted_svm,labels=['Credit reporting, credit repair services, or other personal consumer reports',\n",
    "       'Mortgage', 'Credit card or prepaid card', 'Debt collection',\n",
    "       'Checking or savings account'])\n",
    "print('Classification report for SVM classifier: \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ensemble = result_df['Predicted Ensemble']\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "matrix = classification_report(actual,predicted_ensemble,labels=['Credit reporting, credit repair services, or other personal consumer reports',\n",
    "       'Mortgage', 'Credit card or prepaid card', 'Debt collection',\n",
    "       'Checking or savings account'])\n",
    "print('Classification report for Ensemble classifier: \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_confusion_df = pd.crosstab(result_df['Product'], result_df['Predicted SVM'], rownames=['Actual'], normalize='index')\n",
    "ensemble_confusion_df = pd.crosstab(result_df['Product'], result_df['Predicted Ensemble'], rownames=['Actual'], normalize='index')\n",
    "\n",
    "figure, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,7))\n",
    "# figure, ax1 = plt.subplots(ncols=1, figsize=(7,7))\n",
    "\n",
    "sn.heatmap(SVM_confusion_df, annot=True, cmap=\"YlGnBu\", ax=ax1, cbar=False)\n",
    "sn.heatmap(ensemble_confusion_df, annot=True, cmap=\"YlGnBu\", ax=ax2, cbar=False)\n",
    "ax1.title.set_text(\"SVM\")\n",
    "ax2.title.set_text(\"Ensemble\")\n",
    "ax2.set_yticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
