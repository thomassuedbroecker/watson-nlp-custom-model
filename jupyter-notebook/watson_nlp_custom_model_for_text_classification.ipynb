{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example reuses information from an example called [Text classification using watson NLP](https://developer.ibm.com/tutorials/text-classification-using-watson-nlp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare environment\n",
    "## 1.1 Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Import the requests library\n",
    "\n",
    "# OS\n",
    "import os\n",
    "import wget\n",
    "import pathlib\n",
    "import zipfile\n",
    "import requests\n",
    "# Layout\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "# Data\n",
    "import pandas\n",
    "# Watson\n",
    "import watson_nlp\n",
    "from watson_nlp.workflows.classification import Ensemble\n",
    "from watson_core.data_model.streams.resolver import DataStreamResolver\n",
    "from watson_nlp.blocks.classification.svm import SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wget\n",
    "!pip install os\n",
    "!pip install zipfile\n",
    "!pip install pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Download example data and transform to `csv` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.1. Set the path for the download: Usage of Consumer complaint database to walk you through the process. (https://www.consumerfinance.gov/data-research/consumer-complaints/)\n",
    "URL = \"https://files.consumerfinance.gov/ccdb/complaints.csv.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.2. download the data behind the URL\n",
    "response = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.3. Open the response into a new file called complaints.csv.zip\n",
    "open(\"complaints.csv.zip\", \"wb\").write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.2.4. Verify the path and list the existing variables types, and files\n",
    "directory = os.getcwd()\n",
    "arr = os.listdir(directory)\n",
    "print(\"The variable, arr is of type:\", type(arr))\n",
    "print(\"The variable, directory is of type:\", type(directory))\n",
    "print(\"Directory '% s' created\" % directory)\n",
    "print(\"List '% s' created\" % arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.5. Unzip the downloaded file and verify that the file was unzipped\n",
    "filepath = directory + \"/complaints.csv.zip\"\n",
    "with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory)\n",
    "arr = os.listdir(directory)\n",
    "csv_files = list(pathlib.Path(directory).glob('*.csv'))\n",
    "\n",
    "print(\"Directory '% s' created\" % directory)\n",
    "print(\"List '% s' created\" % arr)\n",
    "print(\"Csv files '% s' created\" % csv_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Optimize the example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.1 Reduce model training time and quick analysis using \"frac\". (https://en.wikipedia.org/wiki/Fractional_part)\n",
    "filepath = directory + \"/complaints.csv\"\n",
    "complaint_df = pandas.read_csv(filepath, error_bad_lines=False)\n",
    "complaint_df = complaint_df.sample(frac=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.2 Look at all of the product groups that are available in the data set because these are the classes that the classifier should predict from a given complaint text.\n",
    "complaint_df['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.3 Filter on the Product categories with a relevant number of samples and remove any other product category from further analysis because many classification algorithms work best if the training samples are equally split across the classes. If the data is unbalanced, algorithms might decide to favor classes with many samples to achieve an overall good result.\n",
    "train_test_df = complaint_df[(complaint_df['Product'] == 'Credit reporting, credit repair services, or other personal consumer reports') | \\\n",
    "                             (complaint_df['Product'] == 'Debt collection') | \\\n",
    "                             (complaint_df['Product'] == 'Mortgage') | \\\n",
    "                             (complaint_df['Product'] == 'Credit card or prepaid card') | \\\n",
    "                             (complaint_df['Product'] == 'Checking or savings account')\n",
    "                            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.4 List the first 5 test entries for the training\n",
    "train_test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.5 Split the data into training and test data (ratio: 80/20).\n",
    "# 80% training data\n",
    "train_orig_df = train_test_df.groupby('Product').sample(frac=0.8, random_state=6)\n",
    "print(\"Training data:\\n\")\n",
    "print(\"Number of training samples:\\n{}\".format(len(train_orig_df)))\n",
    "print(\"Samples by product group:\\n{}\".format(train_orig_df['Product'].value_counts()))\n",
    "\n",
    "# 20% test data\n",
    "test_orig_df = train_test_df.drop(train_orig_df.index)\n",
    "print(\"\\nTest data:\\n\")\n",
    "print(\"Number of test samples:\\n{}\".format(len(test_orig_df)))\n",
    "print(\"Samples by product group:\\n{}\".format(test_orig_df['Product'].value_counts()))\n",
    "\n",
    "# re-index after sampling\n",
    "train_orig_df = train_orig_df.reset_index(drop=True)\n",
    "test_orig_df = test_orig_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.6 Create the data in a JSON format. The training and test data is written to files. \n",
    "def prepare_data(df):\n",
    "       # only the text column and the target label *Product* are needed\n",
    "       df_out = df[['Consumer complaint narrative', 'Product']].reset_index (drop=True)\n",
    "       # rename to the identifiers expected by Watson NLP\n",
    "       df_out = df_out.rename(columns={\"Consumer complaint narrative\": \"text\", 'Product': 'labels'})\n",
    "       # the label column should be an array (although we have only one label per complaint)\n",
    "       df_out['labels'] = df_out['labels'].map(lambda label: [label,])\n",
    "       return df_out\n",
    "\n",
    "train_df = prepare_data(train_orig_df)\n",
    "# Clean all 'NaN'\n",
    "train_df.dropna(subset=['text'], how='all', inplace=True)\n",
    "train_file = directory + \"/train_data.json\"\n",
    "train_df.to_json(train_file, orient='records')\n",
    "\n",
    "test_df = prepare_data(test_orig_df)\n",
    "# Clean all 'NaN'\n",
    "test_df.dropna(subset=['text'], how='all', inplace=True)\n",
    "test_file = directory + \"/test data.json\"\n",
    "test_df.to_json(test_file, orient='records')\n",
    "\n",
    "json_files = list(pathlib.Path(directory).glob('*.json'))\n",
    "print(\"JSON files '% s' created\" % json_files)\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.7 Show labels\n",
    "train_df.explode('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.3.8 Show labels\n",
    "test_df.explode('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.9 Show distribution\n",
    "plotly_template = pio.templates[\"plotly_dark\"]\n",
    "pio.templates[\"plotly_dark_custom\"] = pio.templates[\"plotly_dark\"]\n",
    "\n",
    "complaints_total_figure = px.bar(test_df.explode('labels')['labels'].value_counts())\n",
    "complaints_total_figure.update_layout(template=plotly_template,barmode='stack',title_text='Show test dataset', title_x=0.5)\n",
    "complaints_total_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Load the syntax model and the USE embeddings because the SVM classifier block depends on the syntax block.\n",
    "\n",
    "# Syntax Model\n",
    "syntax_model = watson_nlp.load(watson_nlp.download('syntax_izumo_en_stock'))\n",
    "# USE Embedding Model\n",
    "use_model = watson_nlp.load(watson_nlp.download('embedding_use_en_stock'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Create data streams using several utility methods because classification blocks expect the training data to be in data streams.\n",
    "\n",
    "training_data_file = train_file\n",
    "print (\"Training data file %s\", train_file)\n",
    "\n",
    "# Create datastream from training data\n",
    "data_stream_resolver = DataStreamResolver(target_stream_type=list, expected_keys={'text': str, 'labels': list})\n",
    "training_data = data_stream_resolver.as_data_stream(training_data_file)\n",
    "\n",
    "# Create Syntax stream\n",
    "text_stream, labels_stream = training_data[0], training_data[1]\n",
    "syntax_stream = syntax_model.stream(text_stream)\n",
    "\n",
    "use_train_stream = use_model.stream(syntax_stream, doc_embed_style='raw_text')\n",
    "use_svm_train_stream = watson_nlp.data_model.DataStream.zip(use_train_stream, labels_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2.3 Train the classifier.\n",
    "# This can take several minutes!\n",
    "svm_model = SVM.train(use_svm_train_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Train an ensemble classification model with Watson NLP\n",
    "\n",
    "The ensemble model combines three classification models:\n",
    "\n",
    "* CNN\n",
    "* SVM with TF-IDF features\n",
    "* SVM with USE (Universal Sentence Encoder) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = watson_nlp.download_and_load('text_stopwords_classification_ensemble_en_stock')\n",
    "\n",
    "ensemble_model = Ensemble.train(train_file, 'syntax_izumo_en_stock', 'embedding_glove_en_stock', 'embedding_use_en_stock', stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
